#!/bin/sh

## Select your cluster
###SBATCH --cluster=ub-hpc
#SBATCH --cluster=faculty

## Select your partition
#SBATCH --partition=scavenger --qos=scavenger

#############################
### Set your running time ###
#############################

#SBATCH --time=72:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=4

######################
## Select your GPU ###
######################
## Use snodes command to check their status first
###SBATCH --gres=gpu:tesla_v100-pcie-32gb:2
###SBATCH --gres=gpu:tesla_v100-pcie-16gb:2
###SBATCH --gres=gpu:a100-pcie-40gb:2
#SBATCH --gres=gpu:a100-pcie-80gb:2

### just give my job a single gpu
#SBATCH --gres=gpu:1
### give me a node which has the A100 gpus in it
#​SBATCH --constraint=A100 
###​SBATCH --constraint=V100

######################
## Set your memory ###
######################

#SBATCH --mem=65536
# Memory per node specification is in MB. It is optional. 
# The default limit is 3000MB per core.

## Set your job name
#SBATCH --job-name="neural_astar_training"

## Set the email to receive email
#SBATCH --mail-user=manisury@buffalo.edu
#SBATCH --mail-type=ALL
##SBATCH --requeue
#Specifies that the job will be requeued after a node failure.
#The default is that the job will not be requeued.


## Beginning of your scipt, it is written with shell

echo "SLURM_JOBID="$SLURM_JOBID
echo "SLURM_TIME="$(date +"%Y_%m_%d_%k_%M_%S")

echo "SLURM_JOB_NODELIST"=$SLURM_JOB_NODELIST
echo "SLURM_NNODES"=$SLURM_NNODES
echo "SLURMTMPDIR="$SLURMTMPDIR

echo "working directory = "$SLURM_SUBMIT_DIR


# module use /projects/academic/cwx/modulefiles
# module load python/my-python-3
source ~/.bashrc

##############Choose your own conda environment
conda activate cuda11-6-env
source .venv/bin/activate

## List the module your are using
module list
ulimit -s unlimited

which python
which pytest

nvidia-smi

echo "=========================================="
echo "                GPU Info                  "
echo "=========================================="
python scripts/train_warcraft.py